{
    "name": "root",
    "gauges": {
        "Drive-PPO.Policy.Entropy.mean": {
            "value": 1.4126425981521606,
            "min": 1.4126425981521606,
            "max": 1.4137811660766602,
            "count": 3
        },
        "Drive-PPO.Policy.Entropy.sum": {
            "value": 70612.3515625,
            "min": 29188.92578125,
            "max": 70668.4453125,
            "count": 3
        },
        "Drive-PPO.Step.mean": {
            "value": 499940.0,
            "min": 399943.0,
            "max": 499940.0,
            "count": 3
        },
        "Drive-PPO.Step.sum": {
            "value": 499940.0,
            "min": 399943.0,
            "max": 499940.0,
            "count": 3
        },
        "Drive-PPO.Policy.ExtrinsicValueEstimate.mean": {
            "value": -95.76412200927734,
            "min": -95.76412200927734,
            "max": -90.84455108642578,
            "count": 3
        },
        "Drive-PPO.Policy.ExtrinsicValueEstimate.sum": {
            "value": -98732.8125,
            "min": -98732.8125,
            "max": -38699.77734375,
            "count": 3
        },
        "Drive-PPO.Environment.EpisodeLength.mean": {
            "value": 144.25797101449277,
            "min": 144.25797101449277,
            "max": 147.2106824925816,
            "count": 3
        },
        "Drive-PPO.Environment.EpisodeLength.sum": {
            "value": 49769.0,
            "min": 20377.0,
            "max": 49769.0,
            "count": 3
        },
        "Drive-PPO.Environment.CumulativeReward.mean": {
            "value": -138.5937214207822,
            "min": -138.5937214207822,
            "max": -127.58629781388223,
            "count": 3
        },
        "Drive-PPO.Environment.CumulativeReward.sum": {
            "value": -47814.83389016986,
            "min": -47814.83389016986,
            "max": -17989.667991757393,
            "count": 3
        },
        "Drive-PPO.Policy.ExtrinsicReward.mean": {
            "value": -138.5937214207822,
            "min": -138.5937214207822,
            "max": -127.58629781388223,
            "count": 3
        },
        "Drive-PPO.Policy.ExtrinsicReward.sum": {
            "value": -47814.83389016986,
            "min": -47814.83389016986,
            "max": -17989.667991757393,
            "count": 3
        },
        "Drive-PPO.Losses.PolicyLoss.mean": {
            "value": 0.024296747606713326,
            "min": 0.021869459193355097,
            "max": 0.024296747606713326,
            "count": 3
        },
        "Drive-PPO.Losses.PolicyLoss.sum": {
            "value": 0.12148373803356663,
            "min": 0.04844549470581114,
            "max": 0.12148373803356663,
            "count": 3
        },
        "Drive-PPO.Losses.ValueLoss.mean": {
            "value": 273.57881591796877,
            "min": 221.21343129475912,
            "max": 273.57881591796877,
            "count": 3
        },
        "Drive-PPO.Losses.ValueLoss.sum": {
            "value": 1367.8940795898438,
            "min": 442.42686258951824,
            "max": 1367.8940795898438,
            "count": 3
        },
        "Drive-PPO.Policy.LearningRate.mean": {
            "value": 1.689177436944e-05,
            "min": 1.689177436944e-05,
            "max": 6.31545789485e-05,
            "count": 3
        },
        "Drive-PPO.Policy.LearningRate.sum": {
            "value": 8.44588718472e-05,
            "min": 8.44588718472e-05,
            "max": 0.00017868334043900003,
            "count": 3
        },
        "Drive-PPO.Policy.Epsilon.mean": {
            "value": 0.10563056000000001,
            "min": 0.10563056000000001,
            "max": 0.12105150000000002,
            "count": 3
        },
        "Drive-PPO.Policy.Epsilon.sum": {
            "value": 0.5281528000000001,
            "min": 0.24210300000000004,
            "max": 0.5281528000000001,
            "count": 3
        },
        "Drive-PPO.Policy.Beta.mean": {
            "value": 0.00029096494399999997,
            "min": 0.00029096494399999997,
            "max": 0.0010604698500000002,
            "count": 3
        },
        "Drive-PPO.Policy.Beta.sum": {
            "value": 0.00145482472,
            "min": 0.00145482472,
            "max": 0.0030120939000000003,
            "count": 3
        },
        "Drive-PPO.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 3
        },
        "Drive-PPO.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 3
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1728895870",
        "python_version": "3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]",
        "command_line_arguments": "D:\\projects\\kuliah\\mkom-av\\mkom-av\\venv\\scripts\\mlagents-learn --run-id experiment1 --resume",
        "mlagents_version": "1.1.0",
        "mlagents_envs_version": "1.1.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.4.1+cpu",
        "numpy_version": "1.23.5",
        "end_time_seconds": "1728896909"
    },
    "total": 1039.0411475999863,
    "count": 1,
    "self": 0.020936999993864447,
    "children": {
        "run_training.setup": {
            "total": 0.10451529998681508,
            "count": 1,
            "self": 0.10451529998681508
        },
        "TrainerController.start_learning": {
            "total": 1038.9156953000056,
            "count": 1,
            "self": 2.6945478981069755,
            "children": {
                "TrainerController._reset_env": {
                    "total": 20.732086400006665,
                    "count": 1,
                    "self": 20.732086400006665
                },
                "TrainerController.advance": {
                    "total": 1015.4231644019019,
                    "count": 121299,
                    "self": 2.6288835971208755,
                    "children": {
                        "env_step": {
                            "total": 977.863955905108,
                            "count": 121299,
                            "self": 865.5231151029293,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 110.48966009780997,
                                    "count": 121299,
                                    "self": 7.77360240125563,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 102.71605769655434,
                                            "count": 120643,
                                            "self": 102.71605769655434
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 1.851180704368744,
                                    "count": 121299,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 1016.2323079018679,
                                            "count": 121299,
                                            "is_parallel": true,
                                            "self": 280.6328653912351,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.002459899988025427,
                                                    "count": 1,
                                                    "is_parallel": true,
                                                    "self": 0.00018580001778900623,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0022740999702364206,
                                                            "count": 4,
                                                            "is_parallel": true,
                                                            "self": 0.0022740999702364206
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 735.5969826106448,
                                                    "count": 121299,
                                                    "is_parallel": true,
                                                    "self": 10.271978998702252,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 8.637943502195412,
                                                            "count": 121299,
                                                            "is_parallel": true,
                                                            "self": 8.637943502195412
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 686.9433217053011,
                                                            "count": 121299,
                                                            "is_parallel": true,
                                                            "self": 686.9433217053011
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 29.74373840444605,
                                                            "count": 121299,
                                                            "is_parallel": true,
                                                            "self": 11.558468893665122,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 18.185269510780927,
                                                                    "count": 485196,
                                                                    "is_parallel": true,
                                                                    "self": 18.185269510780927
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 34.930324899672996,
                            "count": 121299,
                            "self": 3.1442751020076685,
                            "children": {
                                "process_trajectory": {
                                    "total": 10.895871097658528,
                                    "count": 121299,
                                    "self": 10.700590697670123,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 0.1952803999884054,
                                            "count": 1,
                                            "self": 0.1952803999884054
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 20.8901787000068,
                                    "count": 11,
                                    "self": 15.932491500134347,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 4.957687199872453,
                                            "count": 330,
                                            "self": 4.957687199872453
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 8.999777492135763e-07,
                    "count": 1,
                    "self": 8.999777492135763e-07
                },
                "TrainerController._save_models": {
                    "total": 0.06589570001233369,
                    "count": 1,
                    "self": 0.021189499995671213,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.04470620001666248,
                            "count": 1,
                            "self": 0.04470620001666248
                        }
                    }
                }
            }
        }
    }
}